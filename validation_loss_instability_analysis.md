# 검증 로스 불안정 원인 분석 (2025-11-18)

## 관찰된 현상

터미널 출력 분석:
- **Epoch 1**: val_loss = 1.2015, train_loss = 0.2530
- **Epoch 2**: val_loss = 0.6180 (개선), train_loss = 0.2026
- **Epoch 3**: val_loss = 0.8474 (악화), train_loss = 0.2023
- **Epoch 4**: val_loss = 0.6126 (개선), train_loss = 0.2079
- **Epoch 5**: val_loss = 1.3892 (크게 악화), train_loss = 0.2129
- **Epoch 6**: val_loss = 1.2436 (악화), train_loss = 0.2320
- **Epoch 7**: val_loss = 1.4229 (크게 악화), train_loss = 0.2354
- **Epoch 8**: val_loss = 0.7646 (개선), train_loss = 0.2897
- **Epoch 9**: val_loss = 0.8385 (악화), train_loss = 0.2373, **학습률 감소** (0.0005 → 0.00025)
- **Epoch 10**: val_loss = 1.1535 (크게 악화), train_loss = 0.3758

**패턴**:
- Train loss는 비교적 안정적으로 감소 (0.2530 → 0.2026 → 0.2023 → ...)
- Val loss는 크게 변동 (0.6180 ↔ 1.4229)
- Val loss가 최고값(0.6126)의 **2배 이상**으로 급격히 증가하는 경우가 빈번
- 학습률 감소 후에도 불안정 지속

## 주요 원인 분석

### 1. **데이터 분포 불일치 (Distribution Shift) - 가장 가능성 높음**

**문제점**:
- 시계열 데이터를 시간 순서대로 분할 (shuffle=False)
- Train/Val/Test가 서로 다른 시장 환경을 포함
- 180일 데이터에서 시장 환경이 크게 변함

**증거**:
- Epoch 2: val_loss = 0.6180 (양호)
- Epoch 5: val_loss = 1.3892 (급격히 악화, 최고값의 2.3배)
- Epoch 7: val_loss = 1.4229 (최고값의 2.3배)
- Epoch 10: val_loss = 1.1535 (최고값의 1.9배)

**왜 발생하나?**
- 비트코인 가격이 180일 동안 크게 변동 (약 51,840개 5분봉)
- Train 데이터: 초반 구간 (예: 상승장 또는 변동성 낮은 구간)
- Val 데이터: 중반 구간 (예: 하락장 또는 변동성 높은 구간)
- 각 구간의 가격 분포, 변동성, 트렌드가 다름
- 모델이 Train 데이터에 과적합되면 Val 데이터에서 성능 급락

**코드 확인**:
```python
# main.py line 127-134
X_train, X_test, y_train, y_test = preprocessor.split_train_test(
    X, y, test_size=0.2, shuffle=False  # 시간 순서대로 분할
)
X_train, X_val, y_train, y_val = preprocessor.split_train_test(
    X_train, y_train, test_size=0.2, shuffle=False  # 시간 순서대로 분할
)
```

### 2. **검증 세트 크기 부족**

**현재 설정**:
- 전체 데이터: 약 51,640개 (warm-up 200개 제거 후)
- 시퀀스 생성 후: 약 51,440개 (window_size=60)
- Train: 약 32,921개 (64%)
- Val: 약 8,230개 (16%)
- Test: 약 10,289개 (20%)

**문제점**:
- 검증 세트가 상대적으로 작음 (전체의 16%)
- 작은 검증 세트는 통계적으로 불안정
- 배치 크기(32)가 상대적으로 커서 검증 세트의 일부만 평가
- 검증 세트 내 특정 구간의 변동성이 크면 loss가 크게 변동

**증거**:
- Val loss가 0.6에서 1.4로 변동 (약 2.3배 차이)
- Train loss는 비교적 안정적 (0.2~0.3 범위)

### 3. **스케일링 문제**

**현재 설정**:
- 특징: RobustScaler (이상치에 강함)
- 타겟: StandardScaler (회귀에 적합)
- **전체 데이터로 스케일러 학습 후 분할**

**잠재적 문제**:
- Train/Val/Test의 분포가 다르면 스케일링이 부적절할 수 있음
- 특히 시계열에서 미래 데이터의 분포가 과거와 다를 수 있음
- Val 데이터가 Train 데이터의 분포 범위를 벗어날 수 있음
- RobustScaler는 중앙값과 IQR을 사용하므로, Train과 Val의 중앙값이 다르면 문제

**코드 확인**:
```python
# data_preprocessor.py line 118-121
X, y, feature_names, y_original = preprocessor.prepare_data(
    df_features, 
    fit_scaler=True  # 전체 데이터로 스케일러 학습
)
# 그 후 분할
X_train, X_test, y_train, y_test = preprocessor.split_train_test(...)
```

**문제 시나리오**:
1. 전체 데이터의 평균/표준편차로 스케일링
2. Train 데이터는 초반 구간 (예: 가격 110,000~115,000)
3. Val 데이터는 중반 구간 (예: 가격 115,000~120,000)
4. Val 데이터가 Train 데이터의 분포 범위를 벗어나면 예측 실패

### 4. **모델 복잡도 vs 데이터 불균형**

**현재 설정**:
- 모델: PatchCNN-BiLSTM
  - CNN filters: [32, 64, 128]
  - LSTM units: 128
  - Dropout: 0.3
  - 파라미터 수: 약 수십만~수백만 개 (추정)
- 학습 데이터: 약 32,921개
- 검증 데이터: 약 8,230개

**문제점**:
- 모델이 너무 복잡해서 작은 검증 세트에 과적합
- 검증 세트가 작아서 통계적으로 불안정
- Dropout(0.3)이 있지만, 여전히 과적합 가능성

**증거**:
- Train loss는 지속적으로 감소
- Val loss는 불안정하게 변동
- Val loss가 Train loss보다 훨씬 큼 (0.6~1.4 vs 0.2~0.3)

### 5. **학습률 및 최적화 문제**

**현재 설정**:
- Learning rate: 0.0005 → 0.00025 (Epoch 9에서 감소)
- Optimizer: Adam (clipnorm=1.0)
- ReduceLROnPlateau: patience=5, factor=0.5

**문제점**:
- 학습률이 너무 높으면 발산 가능
- 학습률이 너무 낮으면 수렴 느림
- Gradient clipping(clipnorm=1.0)이 있지만, 여전히 불안정 가능

**증거**:
- Epoch 9에서 학습률 감소 후에도 불안정 지속
- Epoch 10: val_loss = 1.1535 (여전히 높음)

### 6. **시계열 데이터의 비정상성 (Non-stationarity)**

**문제점**:
- 비트코인 가격은 비정상 시계열 (Non-stationary)
- 평균, 분산, 자기상관이 시간에 따라 변함
- 모델이 과거 패턴을 학습해도 미래 패턴이 다를 수 있음

**증거**:
- Val loss가 크게 변동하는 것은 Val 데이터의 패턴이 Train 데이터와 다르기 때문
- 시장 환경 변화 (상승장 → 하락장, 변동성 변화 등)

### 7. **배치 크기 및 학습 순서**

**현재 설정**:
- Batch size: 32
- Shuffle: False (시계열 데이터)

**문제점**:
- 배치 크기가 상대적으로 작으면 각 배치의 분포가 다를 수 있음
- 시계열 데이터는 시간 순서대로 학습하므로, 특정 구간의 패턴에 과적합 가능

## 종합 분석

**가장 가능성 높은 원인 (우선순위)**:

1. **데이터 분포 불일치 (Distribution Shift)** - 90% 가능성
   - Train/Val이 서로 다른 시장 환경
   - 가장 큰 원인으로 보임

2. **검증 세트 크기 부족** - 70% 가능성
   - 검증 세트가 작아서 통계적으로 불안정
   - 특정 구간의 변동성이 크면 loss가 크게 변동

3. **스케일링 문제** - 60% 가능성
   - 전체 데이터로 스케일링 후 분할
   - Train/Val의 분포가 다르면 문제

4. **모델 복잡도** - 50% 가능성
   - 모델이 복잡해서 과적합 가능
   - Dropout이 있지만 부족할 수 있음

5. **시계열 비정상성** - 40% 가능성
   - 비트코인 가격의 비정상성
   - 시장 환경 변화

## 권장 해결 방안 (참고용, 코드 수정 없음)

1. **Walk-forward 검증 사용**
   - 시간 순서대로 여러 번 검증
   - 각 구간별로 모델 성능 평가

2. **검증 세트 크기 증가**
   - Train:Val:Test = 60:20:20 또는 70:15:15
   - 더 많은 검증 데이터로 안정성 향상

3. **Train 데이터로만 스케일링**
   - 전체 데이터가 아닌 Train 데이터로만 스케일러 학습
   - Val/Test는 Train 스케일러로 변환

4. **모델 복잡도 감소**
   - LSTM units 감소 (128 → 64)
   - CNN filters 감소
   - Dropout 증가 (0.3 → 0.4~0.5)

5. **데이터 정규화 개선**
   - 차분(Differencing) 적용
   - 로그 변환
   - 정상성 확보

6. **Ensemble 방법**
   - 여러 모델의 평균
   - 불안정성 완화

## 결론

검증 로스가 불안정한 주요 원인은 **데이터 분포 불일치**와 **검증 세트 크기 부족**입니다. 시계열 데이터를 시간 순서대로 분할하면서 Train과 Val이 서로 다른 시장 환경을 포함하게 되어, 모델이 Train 데이터에 과적합되면 Val 데이터에서 성능이 급락합니다. 또한 검증 세트가 상대적으로 작아서 통계적으로 불안정하며, 특정 구간의 변동성이 크면 loss가 크게 변동합니다.

